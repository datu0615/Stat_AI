{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20914,"status":"ok","timestamp":1649600268152,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"5HAji7s04eRe","outputId":"298c0f03-f59b-4092-cd72-48854101a77f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22338,"status":"ok","timestamp":1649600290488,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"9V1puuSQM_BU","outputId":"16943fed-5140-42e2-d555-0cf206cf021b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting hanja\n","  Downloading hanja-0.13.3.tar.gz (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 15.5 MB/s \n","\u001b[?25hCollecting pyyaml==5.1.2\n","  Downloading PyYAML-5.1.2.tar.gz (265 kB)\n","\u001b[K     |████████████████████████████████| 265 kB 59.4 MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from hanja) (3.6.4)\n","Collecting pytest-cov\n","  Downloading pytest_cov-3.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: coveralls in /usr/local/lib/python3.7/dist-packages (from hanja) (0.5)\n","Requirement already satisfied: docopt>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from coveralls->hanja) (0.6.2)\n","Requirement already satisfied: requests>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from coveralls->hanja) (2.23.0)\n","Requirement already satisfied: coverage<3.999,>=3.6 in /usr/local/lib/python3.7/dist-packages (from coveralls->hanja) (3.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0.0->coveralls->hanja) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0.0->coveralls->hanja) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0.0->coveralls->hanja) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.0.0->coveralls->hanja) (2.10)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (1.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (8.12.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (1.15.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (1.11.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (57.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->hanja) (21.4.0)\n","Collecting pytest\n","  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n","\u001b[K     |████████████████████████████████| 297 kB 83.0 MB/s \n","\u001b[?25hCollecting coverage[toml]>=5.2.1\n","  Downloading coverage-6.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n","\u001b[K     |████████████████████████████████| 207 kB 72.8 MB/s \n","\u001b[?25h  Downloading coverage-6.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n","\u001b[K     |████████████████████████████████| 207 kB 86.6 MB/s \n","\u001b[?25h  Downloading coverage-6.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n","\u001b[K     |████████████████████████████████| 207 kB 104.4 MB/s \n","\u001b[?25h  Downloading coverage-6.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 101.1 MB/s \n","\u001b[?25h  Downloading coverage-6.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 91.4 MB/s \n","\u001b[?25h  Downloading coverage-6.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 104.4 MB/s \n","\u001b[?25h  Downloading coverage-6.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n","\u001b[K     |████████████████████████████████| 213 kB 86.2 MB/s \n","\u001b[?25h  Downloading coverage-6.0.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (253 kB)\n","\u001b[K     |████████████████████████████████| 253 kB 100.0 MB/s \n","\u001b[?25h  Downloading coverage-6.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n","\u001b[K     |████████████████████████████████| 252 kB 75.3 MB/s \n","\u001b[?25h  Downloading coverage-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n","\u001b[K     |████████████████████████████████| 252 kB 102.0 MB/s \n","\u001b[?25h  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n","\u001b[K     |████████████████████████████████| 242 kB 97.3 MB/s \n","\u001b[?25h  Downloading coverage-5.4-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n","\u001b[K     |████████████████████████████████| 242 kB 89.4 MB/s \n","\u001b[?25h  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n","\u001b[K     |████████████████████████████████| 242 kB 89.2 MB/s \n","\u001b[?25h  Downloading coverage-5.3-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n","\u001b[K     |████████████████████████████████| 229 kB 104.6 MB/s \n","\u001b[?25h  Downloading coverage-5.2.1-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n","\u001b[K     |████████████████████████████████| 229 kB 100.9 MB/s \n","\u001b[?25hINFO: pip is looking at multiple versions of pytest-cov to determine which version is compatible with other requirements. This could take a while.\n","Collecting pytest-cov\n","  Downloading pytest_cov-2.12.1-py2.py3-none-any.whl (20 kB)\n","  Downloading pytest_cov-2.12.0-py2.py3-none-any.whl (20 kB)\n","  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n","  Downloading pytest_cov-2.11.0-py2.py3-none-any.whl (20 kB)\n","  Downloading pytest_cov-2.10.1-py2.py3-none-any.whl (19 kB)\n","  Downloading pytest_cov-2.10.0-py2.py3-none-any.whl (19 kB)\n","  Downloading pytest_cov-2.9.0-py2.py3-none-any.whl (19 kB)\n","  Downloading pytest_cov-2.8.1-py2.py3-none-any.whl (18 kB)\n","  Downloading pytest_cov-2.8.0-py2.py3-none-any.whl (18 kB)\n","  Downloading pytest_cov-2.7.1-py2.py3-none-any.whl (17 kB)\n","  Downloading pytest_cov-2.7.0-py2.py3-none-any.whl (17 kB)\n","  Downloading pytest_cov-2.6.1-py2.py3-none-any.whl (16 kB)\n","  Downloading pytest_cov-2.6.0-py2.py3-none-any.whl (14 kB)\n","  Downloading pytest_cov-2.5.1-py2.py3-none-any.whl (21 kB)\n","Building wheels for collected packages: hanja, pyyaml\n","  Building wheel for hanja (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hanja: filename=hanja-0.13.3-py3-none-any.whl size=128426 sha256=61cbeb42b8de525ff750e85e85f2c042b72ef8402d5a110de67591b7bb93388d\n","  Stored in directory: /root/.cache/pip/wheels/70/08/88/f9cd32ddb92f5c3061cf16f068c842dc558d2f66a9c943b51a\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp37-cp37m-linux_x86_64.whl size=44118 sha256=417f7cbbf7e2bf1a80f41244430c18c10ce59ff70da7cfabe54b61e9d796d78a\n","  Stored in directory: /root/.cache/pip/wheels/23/b9/73/57aaccb6957d94ed63f474b51a9f7f992c5eff4635052c0557\n","Successfully built hanja pyyaml\n","Installing collected packages: pyyaml, pytest-cov, hanja\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed hanja-0.13.3 pytest-cov-2.5.1 pyyaml-5.1.2\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.21.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n","Collecting transformers==3.2\n","  Downloading transformers-3.2.0-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 14.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (3.6.0)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 70.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (1.21.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 76.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.2) (21.3)\n","Collecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 75.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.2) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.2) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.2) (1.1.0)\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.49 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.2.0\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 14.8 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.16.1\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 457 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 74.4 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}],"source":["!pip install hanja\n","!pip install wordcloud\n","!pip install transformers==3.2\n","!pip install tensorflow-addons\n","!pip install konlpy"]},{"cell_type":"markdown","metadata":{"id":"ntscQjKSowSY"},"source":["# MECAB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":987,"status":"ok","timestamp":1649600291471,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"YhLqvWQi6dlL","outputId":"cff8be04-1b71-4921-8396-e42937293eae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Mecab-ko-for-Google-Colab'...\n","remote: Enumerating objects: 109, done.\u001b[K\n","remote: Counting objects: 100% (18/18), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 109 (delta 7), reused 10 (delta 3), pack-reused 91\u001b[K\n","Receiving objects: 100% (109/109), 1.27 MiB | 28.89 MiB/s, done.\n","Resolving deltas: 100% (46/46), done.\n"]}],"source":["! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1649600291471,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"j-953ZZ8_nMz","outputId":"a513faa8-777f-4ba6-da5e-4afc43fd4e2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/Mecab-ko-for-Google-Colab\n"]}],"source":["cd Mecab-ko-for-Google-Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308588,"status":"ok","timestamp":1649600600054,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"RuLVd0p__pjb","outputId":"0b495920-489f-470a-a7d4-95e83456d6cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing konlpy.....\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Done\n","Installing mecab-0.996-ko-0.9.2.tar.gz.....\n","Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n","from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","--2022-04-10 14:18:13--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n","Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c5:2ef4, ...\n","Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=pLYvpyS3A1ah45VZAwr%2FjcFS5Tg%3D&Expires=1649601227&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n","--2022-04-10 14:18:13--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=pLYvpyS3A1ah45VZAwr%2FjcFS5Tg%3D&Expires=1649601227&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n","Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.230.89\n","Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.230.89|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1414979 (1.3M) [application/x-tar]\n","Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n","\n","mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.66MB/s    in 0.5s    \n","\n","2022-04-10 14:18:14 (2.66 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n","\n","Done\n","Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n","Done\n","Change Directory to mecab-0.996-ko-0.9.2.......\n","installing mecab-0.996-ko-0.9.2.tar.gz........\n","configure\n","make\n","make check\n","make install\n","ldconfig\n","Done\n","Change Directory to /content\n","Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n","from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n","--2022-04-10 14:19:33--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n","Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::3403:4be7, ...\n","Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=FPIeyzm2uu9j9EHKBL3L8cVWlPM%3D&Expires=1649601307&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n","--2022-04-10 14:19:33--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=FPIeyzm2uu9j9EHKBL3L8cVWlPM%3D&Expires=1649601307&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n","Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.129.33\n","Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.129.33|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 49775061 (47M) [application/x-tar]\n","Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n","\n","mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  25.8MB/s    in 1.8s    \n","\n","2022-04-10 14:19:35 (25.8 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n","\n","Done\n","Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n","Done\n","Change Directory to mecab-ko-dic-2.1.1-20180720\n","Done\n","installing........\n","configure\n","make\n","make install\n","apt-get update\n","apt-get upgrade\n","apt install curl\n","apt install git\n","bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n","Done\n","Successfully Installed\n","Now you can use Mecab\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n","NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n","블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"]}],"source":["! bash install_mecab-ko_on_colab190912.sh"]},{"cell_type":"markdown","metadata":{"id":"_Jlv7vxarCB4"},"source":["# IMPORT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzxIbpTpvr6G"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import json\n","import matplotlib.pyplot as plt\n","%matplotlib inline \n","from wordcloud import WordCloud\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","import hanja\n","from hanja import hangul\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","\n","from transformers import *\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n","from tensorflow.keras.models import clone_model"]},{"cell_type":"markdown","metadata":{"id":"ls5o_-bCEsYE"},"source":["https://github.com/monologg/KoBERT-Transformers 참고하였습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AmgkdPb482G"},"outputs":[],"source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n"," \n","from transformers import PreTrainedTokenizer\n"," \n"," \n","logger = logging.getLogger(__name__)\n"," \n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n"," \n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n"," \n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n"," \n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n"," \n","SPIECE_UNDERLINE = u'▁'\n"," \n"," \n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n"," \n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n"," \n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n"," \n","        self.max_len_single_sentence = self.max_len - 2  \n","        self.max_len_sentences_pair = self.max_len - 3  \n"," \n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n"," \n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n"," \n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n"," \n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n"," \n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n"," \n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n"," \n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n"," \n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n"," \n","        return outputs\n"," \n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n"," \n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n"," \n","        return new_pieces\n"," \n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n"," \n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n"," \n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n"," \n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A RoBERTa sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n"," \n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n"," \n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n"," \n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n"," \n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A BERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n"," \n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n"," \n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n"," \n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n"," \n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n"," \n","        return out_vocab_model, out_vocab_txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5677,"status":"ok","timestamp":1649600614409,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"2iXF8Qcru5VX","outputId":"28ba9b88-035f-4bae-f308-45a2b84ac6c6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n","\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["PATH = '/content/drive/MyDrive/stat/'\n","\n","\n","train = pd.read_csv(PATH + \"train_all_clean2.csv\", error_bad_lines=False)\n","test = pd.read_csv(PATH + \"test_all_clean2.csv\",error_bad_lines=False)\n","\n","# train['digit_1'] = train['digit_1'].astype('str')\n","# train['digit_2'] = train['digit_2'].astype('str')\n","# train['digit_3'] = train['digit_3'].astype('str')\n","\n","# train['text_all'] = train['text_obj'].map(str)+\" \"+train['text_mthd'].map(str)+\" \"+train['text_deal'].map(str)\n","# test['text_all'] = test['text_obj'].map(str)+\" \"+test['text_mthd'].map(str)+\" \"+test['text_deal'].map(str)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","le1 = LabelEncoder()\n","le1 = le1.fit(train['digit_1'])  \n","train['digit_1'] = le1.transform(train['digit_1'])  \n","\n","le2 = LabelEncoder()\n","le2 = le2.fit(train['digit_2'])  \n","train['digit_2'] = le2.transform(train['digit_2'])  \n","\n","le3 = LabelEncoder()\n","le3 = le3.fit(train['digit_3'])  \n","train['digit_3'] = le3.transform(train['digit_3'])  \n","\n","STOPWORDSPATH =\"/content/drive/MyDrive/stat/stopwords.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":4390,"status":"ok","timestamp":1649600618796,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"Xx22-yI2-ymy","outputId":"3f59af24-9172-4499-96ab-df208ca2ced5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" 가장 긴 문장은 25 개의 단어를, 가장 짧은 문장은 1 개의 단어를 가지고 있습니다.\n"]}],"source":["# 문장 길이 분포도 확인\n","train['doc_len'] = train.text_sum.apply(lambda words: len(words.split()))\n","\n","def plot_doc_lengths(dataframe):\n","    mean_seq_len = np.round(dataframe.doc_len.mean()).astype(int)\n","    sns.distplot(tuple(dataframe.doc_len), hist=True, kde=True, label='Document lengths')\n","    plt.axvline(x=mean_seq_len, color='k', linestyle='--', label=f'Sequence length mean:{mean_seq_len}')\n","    plt.title('Document lengths')\n","    plt.legend()\n","    plt.show()\n","    print(f\" 가장 긴 문장은 {train['doc_len'].max()} 개의 단어를, 가장 짧은 문장은 {train['doc_len'].min()} 개의 단어를 가지고 있습니다.\")\n","\n","plot_doc_lengths(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeteJ2oBJtDQ"},"outputs":[],"source":["## preprocessing\n","punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n","\n","def clean_punc(text, punct, mapping):\n","    for p in mapping:\n","        text = text.replace(p, mapping[p])\n","    \n","    for p in punct:\n","        text = text.replace(p, f' {p} ')\n","    \n","    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n","    for s in specials:\n","        text = text.replace(s, specials[s])\n","    \n","    return text.strip()\n","\n","cleaned_train_corpus = []\n","cleaned_test_corpus = []\n","# train.title = train.title.apply(lambda x : hanja.translate(x, 'substitution'))\n","# test.title = test.title.apply(lambda x : hanja.translate(x, 'substitution'))\n","\n","for sent in train['text_sum']:\n","    cleaned_train_corpus.append(clean_punc(sent, punct, punct_mapping))\n","    \n","for sent in test['text_sum']:\n","    cleaned_test_corpus.append(clean_punc(sent, punct, punct_mapping))\n","\n","\n","def clean_text(texts):\n","    corpus = []\n","    for i in range(0, len(texts)):   \n","\n","        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        review = re.sub(\"[一-龥]\",'', review)\n","        review = re.sub(\"에서\",'', review)\n","        review = re.sub(\"으로\",'', review)\n","        review = re.sub(\"에게\",'', review)\n","        review = re.sub(\"의해\",'', review)\n","        review = re.sub(\"에도\",'', review)\n","        review = re.sub(\"을\",'', review)\n","        review = re.sub(\"및\",'', review)\n","        review = re.sub(\"를\",'', review)\n","        review = re.sub(\"에\",'', review)\n","        review = re.sub(\"하여\",'', review)\n","        review = re.sub(\"대상\",'', review)\n","        review = re.sub(\"등 제공\",'', review)\n","        review = re.sub(\"위해\",'', review)\n","        review = re.sub(\"제공\",'', review)\n","        corpus.append(review)\n","    return corpus\n","\n","basic_preprocessed_train_corpus = clean_text(cleaned_train_corpus)\n","basic_preprocessed_test_corpus = clean_text(cleaned_test_corpus)\n","\n","\n","stopwords = []\n","with open(STOPWORDSPATH) as f:\n","    for line in f:\n","        stopwords.append(line.strip())\n","\n","removed_stopword_train_corpus = []\n","removed_stopword_test_corpus = []\n","\n","for tagged in basic_preprocessed_train_corpus:\n","    tagged=mecab.pos(tagged)\n","    \n","    temp = []\n","    for tag in tagged:\n","        if tag[0] in stopwords or tag[1] not in [\"NNG\", \"NNP\", \"VV\", \"VA\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"XPN\", \"SL\", \"SH\"]:\n","            continue\n","        temp.append(tag[0])\n","\n","    removed_stopword_train_corpus.append(' '.join(temp))\n","    \n","for tagged in basic_preprocessed_test_corpus:\n","    tagged=mecab.pos(tagged)\n","    \n","    temp = []\n","    for tag in tagged:\n","        if tag[0] in stopwords or tag[1] not in [\"NNG\", \"NNP\", \"VV\", \"VA\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"XPN\", \"SL\", \"SH\"]:\n","            continue\n","        temp.append(tag[0])\n","\n","    removed_stopword_test_corpus.append(' '.join(temp))\n","\n","\n","train_text = removed_stopword_train_corpus\n","test_text = removed_stopword_test_corpus\n","train_label1 = np.asarray(train.digit_1)\n","train_label2 = np.asarray(train.digit_2)\n","train_label3 = np.asarray(train.digit_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKyjChFYvoLu"},"outputs":[],"source":["train['text_all_clear'] = train_text\n","test['text_all_clear'] = test_text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1649600775088,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"ayZdkh3jwG_6","outputId":"9e8a926e-6316-4847-ffe9-b42c16c0dc44"},"outputs":[{"data":{"text/plain":["88"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_length = train['text_all_clear'].astype(str).apply(len)\n","train_length.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649600775088,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"qOrw_Bg20zpd","outputId":"0b9e91ad-008a-4989-d501-1305b92bae57"},"outputs":[{"data":{"text/plain":["84"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["test_length = test['text_all_clear'].astype(str).apply(len)\n","test_length.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8eyZF_X1Zzc"},"outputs":[],"source":["train.text_sum = clean_text(train.text_sum)\n","test.text_sum = clean_text(test.text_sum)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2265,"status":"ok","timestamp":1649600807097,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"MPj6TBCou3rO","outputId":"8a4d1c85-1608-47c1-b0b7-6d620c2e9351"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [00:00<00:00, 1489443.44it/s]\n","100%|██████████| 100000/100000 [00:00<00:00, 1565962.02it/s]\n"]}],"source":["train_data_text = list(train['text_sum'])\n","\n","train_clear_text = []\n","\n","for i in tqdm(range(len(train_data_text))):\n","  train_clear_text.append(str(train_data_text[i]).replace('\\\\n', ''))\n","train['text_all_clear'] = train_clear_text\n","\n","\n","train_clear_text = list(train['text_all_clear'])\n","\n","train_clear_text2 = []\n","\n","for text in train_clear_text:\n","  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n","  train_clear_text2.append(temp)\n","train['text_all_clear'] = train_clear_text2\n","\n","\n","test_data_text = list(test['text_sum'])\n","\n","test_clear_text = []\n","\n","for i in tqdm(range(len(test_data_text))):\n","  test_clear_text.append(test_data_text[i].replace('\\\\n', ' '))\n","test['text_all_clear'] = test_clear_text\n","\n","\n","test_clear_text = list(test['text_all_clear'])\n","\n","test_clear_text_final = []\n","\n","for text in test_clear_text:\n","  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n","  test_clear_text_final.append(temp)\n","test['text_all_clear'] = test_clear_text_final"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1649600807590,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"Cf6k12J5Bsxh","outputId":"b089cbb7-93ae-4c34-c207-3f13feb85f28"},"outputs":[{"data":{"text/plain":["84"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_length = train['text_all_clear'].astype(str).apply(len)\n","train_length.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174,"referenced_widgets":["2412646ccf914c2caeee7048ddac95fd","976b24a7ea584d36b3a2473ce3146876","1870d5c2cdde450688857d093b0c94ca","aa7f663e64cb45d184062823b80fbdf6","596b2f14c3fc4c22abc81945c53aac98","c32fe03eaef14014818b95e5b0d842f9","afa995a435f94981969d8db8b6669a69","e0bd5ea7b1c64f98ab3e5f9d71daf09b","b016afd0d7a34acd8079578bd549c175","305e0adf08334517bf20da2350c9bf72","0b3015decb244092b355fccbf5b97e21","e1c9d4e4ed694276adcf25662d9189f8","bc2e0d49aee842918582ee53c6c3b155","a455b4cc1afc419794c5e596afdb0dfd","51e4f71b3ae944af9afd500594e5605b","bcd5892dd6b04d2c9d1bd6b6fc1fbfc7","3df62af249444c0d9fdf9756939333b0","16b34281524c41c1a3249c4d2cbbdd07","9be47cc98f11409287dcccb888e72d59","7b7d6d5c713841028a197a2d92b31577","cb57970db7e04b7baca5bbd66205b54a","c388b893bf5a4701905e644b84754853"]},"executionInfo":{"elapsed":2308,"status":"ok","timestamp":1649600809896,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"4bYfpXXE5Tea","outputId":"1c9b07eb-0171-4c2f-afa2-c23672878ef4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2412646ccf914c2caeee7048ddac95fd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/371k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1c9d4e4ed694276adcf25662d9189f8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n","Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"]}],"source":["tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1649600809896,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"lHfa3Z7MoYeb","outputId":"fa93ab08-cf7e-40cd-cb59-0cf9bd52f634"},"outputs":[{"data":{"text/plain":["19"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(train['digit_1'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649600809896,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"0l9_GuVzfyGU","outputId":"5d0487ca-5640-4111-d08a-55912fd2a1ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_label1 count :  19\n","train_label2 count :  74\n","train_label3 count :  225\n"]}],"source":["print('train_label1 count : ', len(train['digit_1'].unique()))\n","print('train_label2 count : ', len(train['digit_2'].unique()))\n","print('train_label3 count : ', len(train['digit_3'].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YqTZBolw3HL"},"outputs":[],"source":["model_name = 'monologg/kobert'\n","SEED_NUM = 42\n","tf.random.set_seed(SEED_NUM)\n","np.random.seed(SEED_NUM)\n","BATCH_SIZE = 84\n","NUM_EPOCHS = 10\n","VALID_SPLIT = 0.2\n","MAX_LEN = 84\n","NUM_CLASS = 19\n","K_SPLIT = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_xHB5wRmoS9"},"outputs":[],"source":["def bert_tokenizer(sent, MAX_LEN):\n","\n","    encoded_dict = tokenizer.encode_plus(\n","        \n","        text = sent,\n","        add_special_tokens = True,\n","        max_length = MAX_LEN,\n","        pad_to_max_length = True,                                   \n","        return_attention_mask = True,\n","        truncation = True \n","    )\n","\n","\n","    input_id = encoded_dict['input_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","    token_type_id = encoded_dict['token_type_ids']\n","\n","\n","    return input_id, attention_mask, token_type_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"916-2NM6pcy1"},"outputs":[],"source":["train_clean = train[['text_all_clear', 'digit_1']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":766989,"status":"ok","timestamp":1649601577370,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"YC-oZ2O1mxLK","outputId":"d71bde81-19af-49ac-b394-c701a5615529"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","5it [10:13, 122.78s/it]\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","5it [02:33, 30.64s/it]\n"]}],"source":["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","target = train_clean[\"digit_1\"]\n","\n","for fold, (train_idx, val_idx) in tqdm(enumerate(skf.split(train_clean, target))):\n","    train_df = train_clean.iloc[train_idx]\n","    valid_df = train_clean.iloc[val_idx]\n","\n","    input_ids = []\n","    attention_masks = []\n","    token_type_ids = []\n","    train_data_labels = []\n","\n","    for train_sent, train_label in zip(train_df[\"text_all_clear\"], train_df[\"digit_1\"]): \n","        try:\n","            input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n","        \n","            input_ids.append(input_id)\n","            attention_masks.append(attention_mask)\n","            token_type_ids.append(token_type_id)\n","            train_data_labels.append(train_label)\n","        \n","        except Exception as e:\n","            print(e)\n","            print(train_sent)\n","            pass\n","\n","\n","    globals()['train_news_input_ids_{}'.format(fold)] = np.array(input_ids, dtype=int)\n","    globals()['train_news_attention_masks_{}'.format(fold)] = np.array(attention_masks, dtype=int)\n","    globals()['train_news_type_ids_{}'.format(fold)] = np.array(token_type_ids, dtype=int)\n","\n","\n","    globals()['train_news_inputs_{}'.format(fold)] = (locals()['train_news_input_ids_{}'.format(fold)], locals()['train_news_attention_masks_{}'.format(fold)], locals()['train_news_type_ids_{}'.format(fold)])\n","    globals()['train_data_labels_{}'.format(fold)] = np.asarray(train_data_labels, dtype=np.int32)\n","\n","\n","for fold, (train_idx, val_idx) in tqdm(enumerate(skf.split(train_clean, target))):\n","    train_df = train_clean.iloc[train_idx]\n","    valid_df = train_clean.iloc[val_idx]\n","\n","    input_ids = []\n","    attention_masks = []\n","    token_type_ids = []\n","    valid_data_labels = []\n","\n","    for val_sent, val_label in zip(valid_df[\"text_all_clear\"], valid_df[\"digit_1\"]): \n","        try:\n","            input_id, attention_mask, token_type_id = bert_tokenizer(val_sent, MAX_LEN)\n","        \n","            input_ids.append(input_id)\n","            attention_masks.append(attention_mask)\n","            token_type_ids.append(token_type_id)\n","            valid_data_labels.append(val_label)\n","        \n","        except Exception as e:\n","            print(e)\n","            print(val_sent)\n","            pass\n","\n","\n","    globals()['valid_news_input_ids_{}'.format(fold)] = np.array(input_ids, dtype=int)\n","    globals()['valid_news_attention_masks_{}'.format(fold)] = np.array(attention_masks, dtype=int)\n","    globals()['valid_news_type_ids_{}'.format(fold)] = np.array(token_type_ids, dtype=int)\n","\n","\n","    globals()['valid_news_inputs_{}'.format(fold)] = (locals()['valid_news_input_ids_{}'.format(fold)], locals()['valid_news_attention_masks_{}'.format(fold)], locals()['valid_news_type_ids_{}'.format(fold)])\n","    globals()['valid_data_labels_{}'.format(fold)] = np.asarray(valid_data_labels, dtype=np.int32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174,"referenced_widgets":["d34058b17ada4191b175ba69f4fdeb68","bfd9f29c307748fca1c049eda45d3e4a","6e7567df86b94246b3fc784a796648a5","3079454d6d264496a8908bb19c685006","4c0d15242e014d789b65438f93a6bf51","eeb8db04dac44652b5bf6b2f519ae65e","773bd225cf5642dba95789fc4df7f2b5","59777173d98a4705b3ed367dc7a26683","777175abaa7b4f85a66f8eda4b7925c3","58467897096b4a3682e7d3275e50d0a2","953f35388622420fac92c97249b0a9c4","04a7e0d60dbb4698b076b23a6e902961","e9de1b5db0954d49a33d761b61758822","3da3f573902d4af48c853655794b666f","8e555d3a983c42b58c9888dd3de8fbbc","bd7e312d07bb44a3be69bd5bc4fe502a","e890fcbacc414a4fab108360760d729e","8be87ea9935e4cd1812b6d02d2701ae0","d4d84407c04b453daa58e1b1dfeb3abe","c7bf5ef82669417f94416e96ec6d3863","acfdf4f634fe4cdcb58c9869080fb249","123f5ff899864f99b192c28b02a5e2dd"]},"executionInfo":{"elapsed":11297,"status":"ok","timestamp":1649601599990,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"c2F4tq4n0tyu","outputId":"a79f58ea-b076-4621-9a91-98ed52c120ec"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d34058b17ada4191b175ba69f4fdeb68","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/426 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04a7e0d60dbb4698b076b23a6e902961","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["class TFBertClassifier(tf.keras.Model):                                                \n","    def __init__(self, model_name, dir_path, num_class):\n","        super(TFBertClassifier, self).__init__()\n","\n","         \n","        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True) \n","                                                                                                                                    \n","        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n","        # self.classifier을 통해 topic_idx를 전부 분류\n","        self.classifier = tf.keras.layers.Dense(num_class,\n","                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n","                                                name=\"classifier\") \n","\n","\n","    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False): \n","        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output, training=training)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","cls_model = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":28451984,"status":"error","timestamp":1649515182153,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"JD9HD3Uu00NA","outputId":"23a961b0-7f4d-4420-da9d-0639a7077235"},"outputs":[{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1809 - accuracy: 0.9506\n","Epoch 1: val_accuracy improved from -inf to 0.97783, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold0.ckpt\n","9524/9524 [==============================] - 4768s 497ms/step - loss: 0.1809 - accuracy: 0.9506 - val_loss: 0.0764 - val_accuracy: 0.9778\n","Epoch 2/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9818\n","Epoch 2: val_accuracy improved from 0.97783 to 0.97844, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold0.ckpt\n","9524/9524 [==============================] - 4726s 496ms/step - loss: 0.0616 - accuracy: 0.9818 - val_loss: 0.0742 - val_accuracy: 0.9784\n","Epoch 3/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9847\n","Epoch 3: val_accuracy improved from 0.97844 to 0.97853, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold0.ckpt\n","9524/9524 [==============================] - 4724s 496ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.0757 - val_accuracy: 0.9785\n","Epoch 4/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9870\n","Epoch 4: val_accuracy did not improve from 0.97853\n","9524/9524 [==============================] - 4724s 496ms/step - loss: 0.0432 - accuracy: 0.9870 - val_loss: 0.0822 - val_accuracy: 0.9783\n","Epoch 5/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9892\n","Epoch 5: val_accuracy did not improve from 0.97853\n","9524/9524 [==============================] - 4725s 496ms/step - loss: 0.0363 - accuracy: 0.9892 - val_loss: 0.0850 - val_accuracy: 0.9779\n"]},{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9465\n","Epoch 1: val_accuracy improved from -inf to 0.97808, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold1.ckpt\n","9524/9524 [==============================] - 4764s 497ms/step - loss: 0.1972 - accuracy: 0.9465 - val_loss: 0.0766 - val_accuracy: 0.9781\n","Epoch 2/10\n","   2/9524 [..............................] - ETA: 1:12:43 - loss: 0.0943 - accuracy: 0.9881"]},{"ename":"ResourceExhaustedError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-b04467620197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         ) \n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/tf_bert_classifier_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2/Mul_1' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n      self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-26-b04467620197>\", line 41, in <module>\n      callbacks=[es_callback, cp_callback]\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 531, in minimize\n      loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/tf_bert_classifier_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2/Mul_1'\nfailed to allocate memory\n\t [[{{node gradient_tape/tf_bert_classifier_2/tf_bert_model_2/bert/encoder/layer_._11/output/LayerNorm/batchnorm/mul_2/Mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_331591]"]}],"source":["# for i in range(5):\n","#     globals()['cls_model_{}'.format(i)] = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)\n","#     optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","#     metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","#     locals()['cls_model_{}'.format(i)].compile(optimizer=optimizer,\n","#                                 loss=loss,\n","#                                 metrics=[metric])\n","    \n","#     es_callback = EarlyStopping(monitor='val_loss', \n","#                                 mode='min',\n","#                                 min_delta=0.0001, \n","#                                 patience=3,\n","#                                 baseline=0.4\n","#                                  ) \n","\n","#     DATA_OUT_PATH = '/content/drive/MyDrive/stat/model/'\n","#     checkpoint_path = DATA_OUT_PATH +  f'best_model1_84_fold{i}.ckpt'\n","#     checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","#     if os.path.exists(checkpoint_dir):\n","#         print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","#     else:\n","#         os.makedirs(checkpoint_dir, exist_ok=True)\n","#         print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","#     cp_callback = ModelCheckpoint(\n","#         checkpoint_path, \n","#         monitor='val_accuracy',\n","#         verbose=1, \n","#         save_best_only=True, \n","#         save_weights_only=True \n","#         )\n","    \n","#     history = locals()['cls_model_{}'.format(i)].fit(locals()['train_news_inputs_{}'.format(i)], locals()['train_data_labels_{}'.format(i)], \n","#                         validation_data=( locals()['valid_news_inputs_{}'.format(i)], locals()['valid_data_labels_{}'.format(i)]),\n","#                         epochs=NUM_EPOCHS,\n","#                         batch_size=BATCH_SIZE,\n","#                         callbacks=[es_callback, cp_callback]\n","#                         ) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23517708,"status":"ok","timestamp":1649572888451,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"_dETMgNsuSeI","outputId":"9b01875b-8d3e-4e20-aca9-8aff9de305b1"},"outputs":[{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9502\n","Epoch 1: val_accuracy improved from -inf to 0.97870, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold1.ckpt\n","9524/9524 [==============================] - 4741s 494ms/step - loss: 0.1831 - accuracy: 0.9502 - val_loss: 0.0734 - val_accuracy: 0.9787\n","Epoch 2/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9818\n","Epoch 2: val_accuracy improved from 0.97870 to 0.97929, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold1.ckpt\n","9524/9524 [==============================] - 4695s 493ms/step - loss: 0.0627 - accuracy: 0.9818 - val_loss: 0.0709 - val_accuracy: 0.9793\n","Epoch 3/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9846\n","Epoch 3: val_accuracy did not improve from 0.97929\n","9524/9524 [==============================] - 4690s 492ms/step - loss: 0.0521 - accuracy: 0.9846 - val_loss: 0.0725 - val_accuracy: 0.9792\n","Epoch 4/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9869\n","Epoch 4: val_accuracy did not improve from 0.97929\n","9524/9524 [==============================] - 4692s 493ms/step - loss: 0.0441 - accuracy: 0.9869 - val_loss: 0.0769 - val_accuracy: 0.9790\n","Epoch 5/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9888\n","Epoch 5: val_accuracy did not improve from 0.97929\n","9524/9524 [==============================] - 4694s 493ms/step - loss: 0.0374 - accuracy: 0.9888 - val_loss: 0.0808 - val_accuracy: 0.9790\n"]}],"source":["for i in range(5):\n","    if i == 1:\n","        globals()['cls_model_{}'.format(i)] = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)\n","        optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","        locals()['cls_model_{}'.format(i)].compile(optimizer=optimizer,\n","                                loss=loss,\n","                                metrics=[metric])\n","    \n","        es_callback = EarlyStopping(monitor='val_loss', \n","                                mode='min',\n","                                min_delta=0.0001, \n","                                patience=3,\n","                                baseline=0.4\n","                                 ) \n","\n","        DATA_OUT_PATH = '/content/drive/MyDrive/stat/model/'\n","        checkpoint_path = DATA_OUT_PATH +  f'best_model1_84_fold{i}.ckpt'\n","        checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","        if os.path.exists(checkpoint_dir):\n","            print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","        else:\n","            os.makedirs(checkpoint_dir, exist_ok=True)\n","            print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","        cp_callback = ModelCheckpoint(\n","            checkpoint_path, \n","            monitor='val_accuracy',\n","            verbose=1, \n","            save_best_only=True, \n","            save_weights_only=True \n","            )\n","    \n","        history = locals()['cls_model_{}'.format(i)].fit(locals()['train_news_inputs_{}'.format(i)], locals()['train_data_labels_{}'.format(i)], \n","                        validation_data=( locals()['valid_news_inputs_{}'.format(i)], locals()['valid_data_labels_{}'.format(i)]),\n","                        epochs=NUM_EPOCHS,\n","                        batch_size=BATCH_SIZE,\n","                        callbacks=[es_callback, cp_callback]\n","                        ) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23720796,"status":"ok","timestamp":1649540500061,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"rjaVVZ0ExRei","outputId":"4f891899-9fdd-4866-ef06-f4eee0176100"},"outputs":[{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9502\n","Epoch 1: val_accuracy improved from -inf to 0.97825, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold2.ckpt\n","9524/9524 [==============================] - 4782s 498ms/step - loss: 0.1823 - accuracy: 0.9502 - val_loss: 0.0745 - val_accuracy: 0.9783\n","Epoch 2/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9819\n","Epoch 2: val_accuracy improved from 0.97825 to 0.97897, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold2.ckpt\n","9524/9524 [==============================] - 4737s 497ms/step - loss: 0.0617 - accuracy: 0.9819 - val_loss: 0.0726 - val_accuracy: 0.9790\n","Epoch 3/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9848\n","Epoch 3: val_accuracy improved from 0.97897 to 0.97905, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold2.ckpt\n","9524/9524 [==============================] - 4730s 497ms/step - loss: 0.0516 - accuracy: 0.9848 - val_loss: 0.0744 - val_accuracy: 0.9790\n","Epoch 4/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9871\n","Epoch 4: val_accuracy did not improve from 0.97905\n","9524/9524 [==============================] - 4734s 497ms/step - loss: 0.0433 - accuracy: 0.9871 - val_loss: 0.0778 - val_accuracy: 0.9788\n","Epoch 5/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9891\n","Epoch 5: val_accuracy did not improve from 0.97905\n","9524/9524 [==============================] - 4734s 497ms/step - loss: 0.0367 - accuracy: 0.9891 - val_loss: 0.0814 - val_accuracy: 0.9786\n"]}],"source":["for i in range(5):\n","    if i == 2:\n","        globals()['cls_model_{}'.format(i)] = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)\n","        optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","        locals()['cls_model_{}'.format(i)].compile(optimizer=optimizer,\n","                                loss=loss,\n","                                metrics=[metric])\n","    \n","        es_callback = EarlyStopping(monitor='val_loss', \n","                                mode='min',\n","                                min_delta=0.0001, \n","                                patience=3,\n","                                baseline=0.4\n","                                 ) \n","\n","        DATA_OUT_PATH = '/content/drive/MyDrive/stat/model/'\n","        checkpoint_path = DATA_OUT_PATH +  f'best_model1_84_fold{i}.ckpt'\n","        checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","        if os.path.exists(checkpoint_dir):\n","            print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","        else:\n","            os.makedirs(checkpoint_dir, exist_ok=True)\n","            print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","        cp_callback = ModelCheckpoint(\n","            checkpoint_path, \n","            monitor='val_accuracy',\n","            verbose=1, \n","            save_best_only=True, \n","            save_weights_only=True \n","            )\n","    \n","        history = locals()['cls_model_{}'.format(i)].fit(locals()['train_news_inputs_{}'.format(i)], locals()['train_data_labels_{}'.format(i)], \n","                        validation_data=( locals()['valid_news_inputs_{}'.format(i)], locals()['valid_data_labels_{}'.format(i)]),\n","                        epochs=NUM_EPOCHS,\n","                        batch_size=BATCH_SIZE,\n","                        callbacks=[es_callback, cp_callback]\n","                        ) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23696562,"status":"ok","timestamp":1649598146062,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"B0ak9fGPpIaw","outputId":"521f6dc9-4c7f-4d69-da49-077026da9ad3"},"outputs":[{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9503\n","Epoch 1: val_accuracy improved from -inf to 0.97872, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold3.ckpt\n","9524/9524 [==============================] - 4763s 496ms/step - loss: 0.1822 - accuracy: 0.9503 - val_loss: 0.0720 - val_accuracy: 0.9787\n","Epoch 2/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9817\n","Epoch 2: val_accuracy improved from 0.97872 to 0.97958, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold3.ckpt\n","9524/9524 [==============================] - 4722s 496ms/step - loss: 0.0625 - accuracy: 0.9817 - val_loss: 0.0705 - val_accuracy: 0.9796\n","Epoch 3/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9844\n","Epoch 3: val_accuracy improved from 0.97958 to 0.97970, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold3.ckpt\n","9524/9524 [==============================] - 4730s 497ms/step - loss: 0.0521 - accuracy: 0.9844 - val_loss: 0.0721 - val_accuracy: 0.9797\n","Epoch 4/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9869\n","Epoch 4: val_accuracy improved from 0.97970 to 0.98017, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold3.ckpt\n","9524/9524 [==============================] - 4741s 498ms/step - loss: 0.0440 - accuracy: 0.9869 - val_loss: 0.0724 - val_accuracy: 0.9802\n","Epoch 5/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9888\n","Epoch 5: val_accuracy did not improve from 0.98017\n","9524/9524 [==============================] - 4736s 497ms/step - loss: 0.0372 - accuracy: 0.9888 - val_loss: 0.0793 - val_accuracy: 0.9795\n"]}],"source":["for i in range(5):\n","    if i == 3:\n","        globals()['cls_model_{}'.format(i)] = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)\n","        optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","        locals()['cls_model_{}'.format(i)].compile(optimizer=optimizer,\n","                                loss=loss,\n","                                metrics=[metric])\n","    \n","        es_callback = EarlyStopping(monitor='val_loss', \n","                                mode='min',\n","                                min_delta=0.0001, \n","                                patience=3,\n","                                baseline=0.4\n","                                 ) \n","\n","        DATA_OUT_PATH = '/content/drive/MyDrive/stat/model/'\n","        checkpoint_path = DATA_OUT_PATH +  f'best_model1_84_fold{i}.ckpt'\n","        checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","        if os.path.exists(checkpoint_dir):\n","            print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","        else:\n","            os.makedirs(checkpoint_dir, exist_ok=True)\n","            print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","        cp_callback = ModelCheckpoint(\n","            checkpoint_path, \n","            monitor='val_accuracy',\n","            verbose=1, \n","            save_best_only=True, \n","            save_weights_only=True \n","            )\n","    \n","        history = locals()['cls_model_{}'.format(i)].fit(locals()['train_news_inputs_{}'.format(i)], locals()['train_data_labels_{}'.format(i)], \n","                        validation_data=( locals()['valid_news_inputs_{}'.format(i)], locals()['valid_data_labels_{}'.format(i)]),\n","                        epochs=NUM_EPOCHS,\n","                        batch_size=BATCH_SIZE,\n","                        callbacks=[es_callback, cp_callback]\n","                        ) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"A363GLpdQqcv","outputId":"684ff257-90b6-4d33-a483-c3d8bb52dbdc"},"outputs":[{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/stat/model -- Folder already exists \n","\n","Epoch 1/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9503\n","Epoch 1: val_accuracy improved from -inf to 0.97840, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold4.ckpt\n","9524/9524 [==============================] - 4751s 495ms/step - loss: 0.1825 - accuracy: 0.9503 - val_loss: 0.0741 - val_accuracy: 0.9784\n","Epoch 2/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9817\n","Epoch 2: val_accuracy improved from 0.97840 to 0.97933, saving model to /content/drive/MyDrive/stat/model/best_model1_84_fold4.ckpt\n","9524/9524 [==============================] - 4721s 496ms/step - loss: 0.0624 - accuracy: 0.9817 - val_loss: 0.0729 - val_accuracy: 0.9793\n","Epoch 3/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9846\n","Epoch 3: val_accuracy did not improve from 0.97933\n","9524/9524 [==============================] - 4717s 495ms/step - loss: 0.0521 - accuracy: 0.9846 - val_loss: 0.0746 - val_accuracy: 0.9792\n","Epoch 4/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9872\n","Epoch 4: val_accuracy did not improve from 0.97933\n","9524/9524 [==============================] - 4718s 495ms/step - loss: 0.0435 - accuracy: 0.9872 - val_loss: 0.0766 - val_accuracy: 0.9793\n","Epoch 5/10\n","9524/9524 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n","Epoch 5: val_accuracy did not improve from 0.97933\n","9524/9524 [==============================] - 4719s 496ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.0830 - val_accuracy: 0.9787\n"]}],"source":["for i in range(5):\n","    if i == 4:\n","        globals()['cls_model_{}'.format(i)] = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)\n","        optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","        locals()['cls_model_{}'.format(i)].compile(optimizer=optimizer,\n","                                loss=loss,\n","                                metrics=[metric])\n","    \n","        es_callback = EarlyStopping(monitor='val_loss', \n","                                mode='min',\n","                                min_delta=0.0001, \n","                                patience=3,\n","                                baseline=0.4\n","                                 ) \n","\n","        DATA_OUT_PATH = '/content/drive/MyDrive/stat/model/'\n","        checkpoint_path = DATA_OUT_PATH +  f'best_model1_84_fold{i}.ckpt'\n","        checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","        if os.path.exists(checkpoint_dir):\n","            print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","        else:\n","            os.makedirs(checkpoint_dir, exist_ok=True)\n","            print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","        cp_callback = ModelCheckpoint(\n","            checkpoint_path, \n","            monitor='val_accuracy',\n","            verbose=1, \n","            save_best_only=True, \n","            save_weights_only=True \n","            )\n","    \n","        history = locals()['cls_model_{}'.format(i)].fit(locals()['train_news_inputs_{}'.format(i)], locals()['train_data_labels_{}'.format(i)], \n","                        validation_data=( locals()['valid_news_inputs_{}'.format(i)], locals()['valid_data_labels_{}'.format(i)]),\n","                        epochs=NUM_EPOCHS,\n","                        batch_size=BATCH_SIZE,\n","                        callbacks=[es_callback, cp_callback]\n","                        ) "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"kobert_mecab1_kfold.ipynb","provenance":[{"file_id":"12hCD7UZ15IxfjW8pNi3uaOq5fGxIxw4X","timestamp":1626014439720},{"file_id":"14qEgegzHqlPAFj8YYtvH79kVzWRSns2Q","timestamp":1625760424590}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04a7e0d60dbb4698b076b23a6e902961":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9de1b5db0954d49a33d761b61758822","IPY_MODEL_3da3f573902d4af48c853655794b666f","IPY_MODEL_8e555d3a983c42b58c9888dd3de8fbbc"],"layout":"IPY_MODEL_bd7e312d07bb44a3be69bd5bc4fe502a"}},"0b3015decb244092b355fccbf5b97e21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"123f5ff899864f99b192c28b02a5e2dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16b34281524c41c1a3249c4d2cbbdd07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1870d5c2cdde450688857d093b0c94ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0bd5ea7b1c64f98ab3e5f9d71daf09b","max":371391,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b016afd0d7a34acd8079578bd549c175","value":371391}},"2412646ccf914c2caeee7048ddac95fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_976b24a7ea584d36b3a2473ce3146876","IPY_MODEL_1870d5c2cdde450688857d093b0c94ca","IPY_MODEL_aa7f663e64cb45d184062823b80fbdf6"],"layout":"IPY_MODEL_596b2f14c3fc4c22abc81945c53aac98"}},"305e0adf08334517bf20da2350c9bf72":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3079454d6d264496a8908bb19c685006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58467897096b4a3682e7d3275e50d0a2","placeholder":"​","style":"IPY_MODEL_953f35388622420fac92c97249b0a9c4","value":" 426/426 [00:00&lt;00:00, 16.9kB/s]"}},"3da3f573902d4af48c853655794b666f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4d84407c04b453daa58e1b1dfeb3abe","max":368792146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7bf5ef82669417f94416e96ec6d3863","value":368792146}},"3df62af249444c0d9fdf9756939333b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c0d15242e014d789b65438f93a6bf51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e4f71b3ae944af9afd500594e5605b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb57970db7e04b7baca5bbd66205b54a","placeholder":"​","style":"IPY_MODEL_c388b893bf5a4701905e644b84754853","value":" 77.8k/77.8k [00:00&lt;00:00, 256kB/s]"}},"58467897096b4a3682e7d3275e50d0a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"596b2f14c3fc4c22abc81945c53aac98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59777173d98a4705b3ed367dc7a26683":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e7567df86b94246b3fc784a796648a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59777173d98a4705b3ed367dc7a26683","max":426,"min":0,"orientation":"horizontal","style":"IPY_MODEL_777175abaa7b4f85a66f8eda4b7925c3","value":426}},"773bd225cf5642dba95789fc4df7f2b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"777175abaa7b4f85a66f8eda4b7925c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b7d6d5c713841028a197a2d92b31577":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8be87ea9935e4cd1812b6d02d2701ae0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e555d3a983c42b58c9888dd3de8fbbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acfdf4f634fe4cdcb58c9869080fb249","placeholder":"​","style":"IPY_MODEL_123f5ff899864f99b192c28b02a5e2dd","value":" 369M/369M [00:05&lt;00:00, 63.0MB/s]"}},"953f35388622420fac92c97249b0a9c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"976b24a7ea584d36b3a2473ce3146876":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c32fe03eaef14014818b95e5b0d842f9","placeholder":"​","style":"IPY_MODEL_afa995a435f94981969d8db8b6669a69","value":"Downloading: 100%"}},"9be47cc98f11409287dcccb888e72d59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a455b4cc1afc419794c5e596afdb0dfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9be47cc98f11409287dcccb888e72d59","max":77779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b7d6d5c713841028a197a2d92b31577","value":77779}},"aa7f663e64cb45d184062823b80fbdf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_305e0adf08334517bf20da2350c9bf72","placeholder":"​","style":"IPY_MODEL_0b3015decb244092b355fccbf5b97e21","value":" 371k/371k [00:00&lt;00:00, 551kB/s]"}},"acfdf4f634fe4cdcb58c9869080fb249":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afa995a435f94981969d8db8b6669a69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b016afd0d7a34acd8079578bd549c175":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc2e0d49aee842918582ee53c6c3b155":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3df62af249444c0d9fdf9756939333b0","placeholder":"​","style":"IPY_MODEL_16b34281524c41c1a3249c4d2cbbdd07","value":"Downloading: 100%"}},"bcd5892dd6b04d2c9d1bd6b6fc1fbfc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7e312d07bb44a3be69bd5bc4fe502a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd9f29c307748fca1c049eda45d3e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb8db04dac44652b5bf6b2f519ae65e","placeholder":"​","style":"IPY_MODEL_773bd225cf5642dba95789fc4df7f2b5","value":"Downloading: 100%"}},"c32fe03eaef14014818b95e5b0d842f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c388b893bf5a4701905e644b84754853":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7bf5ef82669417f94416e96ec6d3863":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb57970db7e04b7baca5bbd66205b54a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d34058b17ada4191b175ba69f4fdeb68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfd9f29c307748fca1c049eda45d3e4a","IPY_MODEL_6e7567df86b94246b3fc784a796648a5","IPY_MODEL_3079454d6d264496a8908bb19c685006"],"layout":"IPY_MODEL_4c0d15242e014d789b65438f93a6bf51"}},"d4d84407c04b453daa58e1b1dfeb3abe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bd5ea7b1c64f98ab3e5f9d71daf09b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1c9d4e4ed694276adcf25662d9189f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc2e0d49aee842918582ee53c6c3b155","IPY_MODEL_a455b4cc1afc419794c5e596afdb0dfd","IPY_MODEL_51e4f71b3ae944af9afd500594e5605b"],"layout":"IPY_MODEL_bcd5892dd6b04d2c9d1bd6b6fc1fbfc7"}},"e890fcbacc414a4fab108360760d729e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9de1b5db0954d49a33d761b61758822":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e890fcbacc414a4fab108360760d729e","placeholder":"​","style":"IPY_MODEL_8be87ea9935e4cd1812b6d02d2701ae0","value":"Downloading: 100%"}},"eeb8db04dac44652b5bf6b2f519ae65e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}